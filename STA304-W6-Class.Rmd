---
title: "Week 6 Class"
author: "Samantha-Jo Caetano"
date: "25/10/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Checking Model Assumptions

Recall there are two different types of models that we have covered in STA304: linear regression and logistic regression. Previously we have also talked about how to use R to estimate these models via both Frequentist and Bayesian techniques. Below we will discuss how to check model assumptions for both linear and logistic regression. The data we will use is the starwars data, introduced a few lectures ago:

```{r}
library(tidyverse)
my_data <- starwars

glimpse(my_data)
head(my_data)
```

## Linear Regression

Recall, for linear regression we need to check the following assumptions:

- Linearity: The relationship between X and the mean of Y is linear.
- Homogeneity of variance: The variance of residual is the same for any value of X.
- Independence: Observations are independent of each other.
- Normality: For any fixed value of X, Y is normally distributed.

Here we will show some simple ways to check some of these assumptions. Let's assume we are building a simple linear regression model where we are trying to use mass to predict height. The model we are trying to estimate is:

$$y = \beta_0 + \beta_1x + \epsilon$$


Let's make a scatterplot. I am using `plot(x,y)` in base R, but you can also do this via `ggplot2` specifying `geom_point()`.

```{r}
plot(my_data$mass, my_data$height)

summary(my_data$mass)
```

It looks like there is an outlier (a very very dense character) with mass around 1400. Let's filter this out for now.

```{r}
my_data_clean <- my_data %>% filter(mass < 600)

plot(my_data_clean$mass, my_data_clean$height)
```

This looks fairly linear. There does appear to be a slight curve, maybe a log trnasformation might help to improve linearity. For now, we will leave it.

Let's fit the model.

```{r}
SLR_model <- lm(height~mass, my_data_clean)
summary(SLR_model)
```

Now, let's look at the assumptions via diagnostic plots:
```{r}
par(mfrow = c(2, 2))
plot(SLR_model)
```

### Linearity of the data:
```{r}
par(mfrow = c(1, 1))
plot(SLR_model, 1)
```

Ideally, the residual plot will show no fitted pattern. That is, the red line should be approximately horizontal at zero.

### Homogeneity of variance:
```{r}
par(mfrow = c(1, 1))
plot(SLR_model, 3)
```

This plot shows if residuals are spread equally along the ranges of predictors. It’s good if you see a horizontal line with equally spread points. In our example, this is not the case.

It can be seen that the variability (variances) of the residual points increases with the value of the fitted outcome variable, suggesting non-constant variances in the residuals errors (or heteroscedasticity).

A possible solution to reduce the heteroscedasticity problem is to use a log or square root transformation of the outcome variable (y).

### Normality of residuals

```{r}
par(mfrow = c(1, 1))
plot(SLR_model, 2)
```

We can check the QQ plot of residuals to visually check normality. Residuals should approximately follow along the diagonal straight line.

In our example, most of the points fall approximately along this reference line, so we can loosely assume normality.

### Other diagnostics

Outliers, high levarage points and influential values can be assessed from using `plot(model, 4)` and `plot(model, 5)`. For more in depth on these diagnostics please review your STA302 notes.

### Transformation

Based on the scatterplot and some of the diagnostic plots, it might be appropriate to perform a log transformation to the `mass` variable in the data. 

```{r}
SLR_model_log <- lm(height~log(mass), my_data_clean)
summary(SLR_model_log)

plot(SLR_model_log,1)
plot(SLR_model_log,2)
plot(SLR_model_log,3)
```

This looks better.



## Logistic Regression

Recall, for logistic regression we need to check the following assumptions:

- outcome is binary
- linearity in the logit for continuous variables
- absence of multicollinearity
- lack of strongly influential outliers

Here we will show some simple ways to check some of these assumptions. Let's assume we are building a simple logistic regression model where we are trying to use mass and sex to predict whether a character is above 180cm tall. The model we are trying to estimate is:

$$log\Big(\frac{p}{1-p}\Big) = \beta_0 + \beta_1x_1 + \beta_2x_2 $$
Let's create the outcome variable (ensuring it's binary):
```{r}
my_data_clean <- my_data_clean %>% 
  mutate(height_over180 = case_when(height > 180 ~ 1,
                                    height <=180 ~ 0))
```

```{r}
plot(my_data_clean$mass, my_data_clean$height_over180)
```


Now let's create the model:
```{r}
logit_model <- glm(height_over180~mass, my_data_clean, family=binomial)
summary(logit_model)
```

### Linearity of logit with continuous variables

Recall, we are modelling $log\Big(\frac{p}{1-p}\Big) = \beta_0 + \beta_1x $. So, we want to check that $log\Big(\frac{p}{1-p}\Big)$ is linearly related to mass. Let's just familliarize ourselves with this, by using the `predict()` function to make a new variable of the predicted value of $log\Big(\frac{p}{1-p}\Big)$ for each character based on their mass and plot this. Note: it should be (perfectly linear).

First, we can look at the predicted probabilities.

```{r}
my_data_clean <- my_data_clean %>% mutate(predicted_prob=predict(logit_model, type="response"))

my_data_clean %>% ggplot(aes(x=mass, y=predicted_prob, y=height_over180)) + 
  geom_point() 
```

Now let's modify the code to look at the predicted log odds.

```{r}
my_data_clean <- my_data_clean %>% mutate(predicted_log_odds=predict(logit_model))

my_data_clean %>% ggplot(aes(x=mass, y=predicted_log_odds)) + 
  geom_point()
```

Note, the above does NOT check the linearity assumption, because it is "cyclic", we are plugging different masses into the same equation, so of course the plot will look perfectly linear. There are tests available (e.g., the Box Tidwell test) to test the linearity assumption. The Box-Tidwell Test checks this assumption by testing whether the logit transform is a linear function of the predictor, effectively by adding the non-linear transform of the original predictor as an interaction term to test if this addition made no better prediction. 

Box-Tidwell has: $H_0:$ linearity between continuous variable and log-odds vs. $H_A:$ non-linearity between continuous variable and log-odds.

```{r}
library(car) # boxTidwell is in the car package
boxTidwell(height_over180 ~ mass, data=my_data_clean)
```


### Checking influential values

Influential values are extreme individual data points that can alter the quality of the logistic regression model.

The most extreme values in the data can be examined by visualizing the Cook’s distance values. Here we label the top 4 largest values::

```{r}
plot(logit_model, id.n = 4,  which = 4)
```

Note that, not all outliers are influential observations. To check whether the data contains potential influential observations, the standardized residual error can be inspected. Data points with an absolute standardized residuals above 3 represent possible outliers and may deserve closer attention.

When you have outliers some potential options are to: Remove the outliers/concerning points, transform the data (e.g., log scale), use alternative methods (non parametric).

### Multicollinearity

Multicollinearity corresponds to a situation where the data contain highly correlated predictor variables. In this case our model only has one predictor. But if you wanted to look into this you could use the R function vif() [in the car package], to compute the variance inflation factors:

```{r}
vif(logit_model) # won't work because only one predictor

big_model <- glm(height_over180 ~ birth_year+sex+mass, data=my_data_clean, family=binomial)
vif(big_model)
```

As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.

\newpage 

# Model Selection 

Again, to select a model we really should be using some practical justification and I often recommend against using automated selection techniques as they cannot replace the human contribution to model selection.

In this section we will demonstrate two different types of model selection techniques: forward selection and AIC; to select a model to predict a characters height. There are many many many ways to select a model (see slides).

## Forward Selection

Forward selection essentially slowly adds in predictors based off their p-value. Start with no predictors, then add them in one by one based on whichever has the smallest p-value (below 0.05). In this example, we will stop at a maximum 3 predictors. 

Note: this gets tricky when we have multiple 3+ category predictor variables.

Note: There is also a selection technique called backward selection which has a model with all predictors and then removes predictors one by one based on largest p-values. There is also a "stepwise selection" which iterates between forward and backward selection.

### First Predictor

This might be easiest if we can grab the p-value from the model. Let's use summary(model)$coefficients[,4]  to do this.
```{r}
summary(lm(height ~ mass, data=my_data))

summary(lm(height ~ mass, data=my_data))$coefficients[,4]
```


Now let's run this for all predictors:

```{r}

```


```{r}

```



### Second predictor



```{r}

```



### Third predictor


```{r}

```

 

### R Packages

Please note that this section was completed mundanely, there are packages and functions already developed to perform forward, backward and stepwise selection automatically, based on different criterion (i.e., not necessarily via p-value).


# AIC 

The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. AIC is calculated from the number of predictors and the MLE of the model. The best-fit model according to AIC is the one that explains the greatest amount of variation of the response using the fewest possible independent variables.

Once you’ve settled on several possible models, you can use AIC to compare them. Lower AIC scores are better, and AIC penalizes models that use more parameters. So if two models explain the same amount of variation, the one with fewer parameters will have a lower AIC score and will be the better-fit model.

 
## Example

I want to predict height from the following pool of predictors: mass, gender and birth year. There are ? possible models to have:

???

Let's use AIC to select which model to use. This might be easiest if we can grab the AIC from the model. Let's use `AIC(summary(model))`  to do this.

```{r}

```


Note: this approach is similar to using the BIC, R-squared or R-squared adjusted criterion (but for R-squared adjusted we want to maximization.

## Other Model Selection Techniques

Recall that there are many other criteria to use to select your model.



